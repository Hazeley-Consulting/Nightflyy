{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbf23140-f68a-4f35-8494-e1cdfe076538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e72c3a1-7720-46f8-a3d6-00881ab1c1bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68256850-f901-4a1f-a21c-03d82e26b174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a200cddb-15ca-4643-80a9-bd6454f8af58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "591ed2cc-44e6-45dd-9447-6b7bf012a937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f22d4af-06ce-4086-a9ba-0940200465b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import base64\n",
    "import json\n",
    "\n",
    "def load_bigquery_tables_to_dfs(key_base64: str, project_id: str, parent_project_id: str):\n",
    "    \"\"\"\n",
    "    Authenticates to BigQuery using a base64-encoded service account key,\n",
    "    lists all tables in all datasets, and loads each into Spark as a\n",
    "    DataFrame with the name `df_<table_name>`, selecting only\n",
    "    document_id, timestamp, and data if they exist.\n",
    "    \"\"\"\n",
    "    # Step 1: Set up BigQuery client\n",
    "    key_json = json.loads(base64.b64decode(key_base64).decode(\"utf-8\"))\n",
    "    credentials = service_account.Credentials.from_service_account_info(key_json)\n",
    "    client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "\n",
    "    # Step 2: List all datasets\n",
    "    datasets = list(client.list_datasets())\n",
    "\n",
    "    if not datasets:\n",
    "        print(f\"No datasets found in project {project_id}\")\n",
    "        return\n",
    "\n",
    "    # Step 3: Loop through all tables in each dataset\n",
    "    for dataset in datasets:\n",
    "        dataset_id = dataset.dataset_id\n",
    "        tables = list(client.list_tables(dataset_id))\n",
    "\n",
    "        if not tables:\n",
    "            print(f\"No tables found in dataset {dataset_id}\")\n",
    "            continue\n",
    "\n",
    "        for table in tables:\n",
    "            table_id = table.table_id\n",
    "            try:\n",
    "                df = spark.read \\\n",
    "                    .format(\"bigquery\") \\\n",
    "                    .option(\"credentials\", key_base64) \\\n",
    "                    .option(\"project\", project_id) \\\n",
    "                    .option(\"parentProject\", parent_project_id) \\\n",
    "                    .option(\"dataset\", dataset_id) \\\n",
    "                    .option(\"table\", table_id) \\\n",
    "                    .option(\"viewsEnabled\", \"true\") \\\n",
    "                    .load()\n",
    "\n",
    "                # Select only desired columns if they exist\n",
    "                desired_columns = [col for col in ['document_id', 'timestamp', 'data'] if col in df.columns]\n",
    "                df_selected = df.select(*desired_columns)\n",
    "\n",
    "                # Dynamically assign variable in the notebook context\n",
    "                globals()[f\"df_{table_id}\"] = df_selected\n",
    "                print(f\"‚úÖ Loaded: df_{table_id} ({dataset_id}.{table_id})\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to load {dataset_id}.{table_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b2c5c9-7b83-439e-aa9e-351069d96445",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_uc_bronze(df: DataFrame, table_name: str):\n",
    "    \"\"\"\n",
    "    Writes the given DataFrame to Unity Catalog at nightflyy.bronze.<table_name>.\n",
    "    Creates the table if it doesn't exist, otherwise appends.\n",
    "    \"\"\"\n",
    "    full_table_name = f\"nightflyy.bronze.{table_name}\"\n",
    "\n",
    "    # Write to UC with append mode\n",
    "    df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(full_table_name)\n",
    "\n",
    "    print(f\"üì• Written to Unity Catalog table ‚Üí {full_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa182513-50de-4ed4-8d4a-4a2f38f63675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import base64\n",
    "import json\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "def parse_and_expand_json_data(df: DataFrame, sample_limit=20) -> DataFrame:\n",
    "    \"\"\"Infers schema from sampled JSON in 'data' column and expands it.\"\"\"\n",
    "    sample_jsons = df.select(\"data\") \\\n",
    "        .filter(col(\"data\").isNotNull()) \\\n",
    "        .limit(sample_limit) \\\n",
    "        .collect()\n",
    "\n",
    "    schemas = []\n",
    "\n",
    "    def dict_to_row(d):\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, dict):\n",
    "                d[k] = Row(**v)\n",
    "            elif isinstance(v, list) and all(isinstance(i, dict) for i in v):\n",
    "                d[k] = [Row(**i) for i in v]\n",
    "        return Row(**d)\n",
    "\n",
    "    for row in sample_jsons:\n",
    "        try:\n",
    "            sample_dict = json.loads(row[\"data\"])\n",
    "            row_obj = dict_to_row(sample_dict)\n",
    "            df_sample = spark.createDataFrame([row_obj])\n",
    "            schemas.append(df_sample.schema)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping invalid JSON: {e}\")\n",
    "\n",
    "    if not schemas:\n",
    "        raise ValueError(\"No valid schemas found in sample.\")\n",
    "\n",
    "    def merge_struct_types(schema1: StructType, schema2: StructType) -> StructType:\n",
    "        merged_fields = {}\n",
    "        for field in schema1.fields + schema2.fields:\n",
    "            if field.name not in merged_fields:\n",
    "                merged_fields[field.name] = field\n",
    "        return StructType(list(merged_fields.values()))\n",
    "\n",
    "    merged_schema = schemas[0]\n",
    "    for schema in schemas[1:]:\n",
    "        merged_schema = merge_struct_types(merged_schema, schema)\n",
    "\n",
    "    return df.withColumn(\"data_parsed\", from_json(col(\"data\"), merged_schema)) \\\n",
    "             .select(\"document_id\", \"timestamp\", \"data_parsed.*\")\n",
    "\n",
    "\n",
    "def load_and_expand_bigquery_tables(key_base64: str, project_id: str, parent_project_id: str):\n",
    "    \"\"\"\n",
    "    Loads all BigQuery tables and expands the JSON 'data' column for each.\n",
    "    Produces DataFrames named `df_expanded_<table_name>`.\n",
    "    \"\"\"\n",
    "    key_json = json.loads(base64.b64decode(key_base64).decode(\"utf-8\"))\n",
    "    credentials = service_account.Credentials.from_service_account_info(key_json)\n",
    "    client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "\n",
    "    datasets = list(client.list_datasets())\n",
    "    if not datasets:\n",
    "        print(f\"No datasets found in project {project_id}\")\n",
    "        return\n",
    "\n",
    "    for dataset in datasets:\n",
    "        dataset_id = dataset.dataset_id\n",
    "        tables = list(client.list_tables(dataset_id))\n",
    "\n",
    "        for table in tables:\n",
    "            table_id = table.table_id\n",
    "            try:\n",
    "                df = spark.read \\\n",
    "                    .format(\"bigquery\") \\\n",
    "                    .option(\"credentials\", key_base64) \\\n",
    "                    .option(\"project\", project_id) \\\n",
    "                    .option(\"parentProject\", parent_project_id) \\\n",
    "                    .option(\"dataset\", dataset_id) \\\n",
    "                    .option(\"table\", table_id) \\\n",
    "                    .option(\"viewsEnabled\", \"true\") \\\n",
    "                    .load()\n",
    "\n",
    "                if not all(col in df.columns for col in ['document_id', 'timestamp', 'data']):\n",
    "                    print(f\"‚ö†Ô∏è Skipping {table_id} ‚Äî missing required columns.\")\n",
    "                    continue\n",
    "\n",
    "                df_trimmed = df.select(\"document_id\", \"timestamp\", \"data\")\n",
    "                df_expanded = parse_and_expand_json_data(df_trimmed)\n",
    "                globals()[f\"df_expanded_{table_id}\"] = df_expanded\n",
    "                print(f\"‚úÖ Loaded and expanded: df_expanded_{table_id}\")\n",
    "                write_to_uc_bronze(df_expanded, table_id)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing {dataset_id}.{table_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff02a210-ed7f-45d4-81d9-b0828967203e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6847efe0-1dfb-45f0-b799-4832b4ea11a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bea5354-5c3f-45b3-a387-571b12ee7948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import base64, json\n",
    "\n",
    "b64_key = dbutils.secrets.get(scope=\"shared-secrets\", key=\"bq-key-json-base64\")\n",
    "key_json = base64.b64decode(b64_key).decode(\"utf-8\")\n",
    "key_dict = json.loads(key_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e0cad9-bd30-4b52-ab55-220d397eb3ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "key_str = dbutils.secrets.get(scope=\"bq-creds\", key=\"bq-key-json\")\n",
    "\n",
    "# Try to parse the secret as JSON\n",
    "try:\n",
    "    key_dict = json.loads(key_str)\n",
    "    key_json = json.dumps(key_dict)  # flatten to one-line string\n",
    "    print(\"‚úÖ Secret is valid and parsed.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"‚ùå Secret is not valid JSON.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bfc4d68-e6a7-495f-a0da-8063807631ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "# Get JSON from secret\n",
    "key_str = dbutils.secrets.get(scope=\"bq-creds\", key=\"bq-key-json\")\n",
    "\n",
    "# Validate + base64 encode the key\n",
    "key_json = json.dumps(json.loads(key_str))\n",
    "key_base64 = base64.b64encode(key_json.encode(\"utf-8\")).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9cfbca9-a3cf-455c-8d26-4f8c1d553ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_bigquery_tables_to_dfs(\n",
    "    key_base64=key_base64,\n",
    "    project_id=\"slydeapp-dc745\",\n",
    "    parent_project_id=\"slydeapp-dc745\"\n",
    ")\n",
    "\n",
    "# Now you can access df_<table_name>, e.g.:\n",
    "# df_events_raw_latest.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "411faf2f-82db-44f3-a8fa-a491a8b50f06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_and_expand_bigquery_tables(\n",
    "    key_base64=key_base64,\n",
    "    project_id=\"slydeapp-dc745\",\n",
    "    parent_project_id=\"slydeapp-dc745\"\n",
    ")\n",
    "\n",
    "# Now access: df_expanded_events_raw_latest, df_expanded_<other_table_name>, etc."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BigQuery | Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
